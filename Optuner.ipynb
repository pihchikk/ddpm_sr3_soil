{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboardX'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mMetrics\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_logger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbLogger\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboardX\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. Проверьте код в ячейках, чтобы определить возможную причину сбоя. Щелкните <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">здесь</a> для получения дополнительных сведений. Подробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import data as Data\n",
    "import model as Model\n",
    "import argparse\n",
    "import logging\n",
    "import core.logger as Logger\n",
    "import core.metrics as Metrics\n",
    "from core.wandb_logger import WandbLogger\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import gc\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "def calculate_ssim(sr_img, hr_img):\n",
    "    if sr_img.ndim == 3:\n",
    "        sr_img = cv2.cvtColor(sr_img, cv2.COLOR_BGR2GRAY)\n",
    "    if hr_img.ndim == 3:\n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    sr_img = sr_img.astype(float)\n",
    "    hr_img = hr_img.astype(float)\n",
    "\n",
    "    ssim_value, _ = ssim(sr_img, hr_img, full=True)\n",
    "\n",
    "    return ssim_value\n",
    "\n",
    "\n",
    "class EmaLRScheduler:\n",
    "    def __init__(self, optimizer, ema_decay):\n",
    "        self.ema_decay = ema_decay\n",
    "\n",
    "    def get_shadow_lr(self):\n",
    "        return self.ema_decay\n",
    "    \n",
    "\n",
    "def create_directories(opt):\n",
    "    os.makedirs(opt['path']['log'], exist_ok=True)\n",
    "    os.makedirs(opt['path']['results'], exist_ok=True)\n",
    "    os.makedirs(opt['path']['tb_logger'], exist_ok=True)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    print(\"Objective function called.\")\n",
    "    print(os.getcwd())\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Modify the optuna_config with the sampled hyperparameters from the trial\n",
    "    # optuna_config[\"model\"][\"unet\"][\"inner_channel\"] = trial.suggest_int(\"inner_channel\", 32, 128)\n",
    "    #optuna_config[\"model\"][\"unet\"][\"channel_multiplier\"] = trial.suggest_categorical(\"channel_multiplier\", [[1, 2, 4],[1, 2, 4, 8, 16]])\n",
    "    #optuna_config[\"model\"][\"unet\"][\"res_blocks\"] = trial.suggest_int(\"norm_groups\", 16, 32, step = 16)\n",
    "    #optuna_config[\"model\"][\"unet\"][\"attn_res\"] = trial.suggest_categorical(\"attn_res\", [[16], [32]])\n",
    "    #optuna_config[\"model\"][\"unet\"][\"res_blocks\"] = trial.suggest_int(\"res_blocks\", 2, 6, step = 2)\n",
    "    #optuna_config[\"model\"][\"unet\"][\"dropout\"] = trial.suggest_float(\"dropout\", 0.15, 0.5)\n",
    "\n",
    "    optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"schedule\"] = trial.suggest_categorical(['quad', 'linear', 'warmup10', 'warmup50', 'const', 'cosine'])\n",
    "    optuna_config[\"model\"][\"beta_schedule\"][\"val\"][\"schedule\"] = optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"schedule\"]\n",
    "\n",
    "    #optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"linear_start\"] = trial.suggest_loguniform(\"linear_start_train\", 1e-8, 1e-5)\n",
    "    #optuna_config[\"model\"][\"beta_schedule\"][\"val\"][\"linear_start\"] = optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"linear_start\"] #trial.suggest_loguniform(\"linear_start_val\", 1e-9, 1e-6)\n",
    "\n",
    "    #optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"linear_end\"] = trial.suggest_loguniform(\"linear_end_train\", 5e-3, 5e-1)\n",
    "    #optuna_config[\"model\"][\"beta_schedule\"][\"val\"][\"linear_end\"] = optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"linear_end\"] #trial.suggest_loguniform(\"linear_end_val\", 1e-4, 1e-1)\n",
    "\n",
    "    #optuna_config[\"train\"][\"optimizer\"][\"lr\"] = trial.suggest_loguniform(\"lr\", 5e-5, 5e-3)\n",
    "    #optuna_config[\"train\"][\"optimizer\"][\"type\"] = trial.suggest_categorical(\"type_optimizer\", [\"adam\"]) # ['SGD']\n",
    "\n",
    "    #optuna_config[\"train\"][\"optimizer\"][\"finetune_norm\"] = trial.suggest_categorical(\"finetune_norm\", [False, True])\n",
    "    #optuna_config[\"train\"][\"optimizer\"][\"which_model_G\"] = trial.suggest_categorical(\"which_model_G\", [\"sr3\"])\n",
    "\n",
    "    optuna_config[\"path\"][\"log\"] = f\"experiments/logs/trial_{trial.number}\"\n",
    "    optuna_config[\"path\"][\"results\"] = f\"experiments/results/trial_{trial.number}\"\n",
    "    optuna_config[\"path\"][\"tb_logger\"] = f\"experiments/tb_logs/trial_{trial.number}\"\n",
    "\n",
    "    opt = optuna_config\n",
    "\n",
    "    create_directories(opt)\n",
    "\n",
    "    # logging\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    logger = Logger.setup_logger(None, opt['path']['log'], 'train', level=logging.INFO, screen=True)\n",
    "    logger_val = logging.getLogger('val')  # Retrieve the logger when needed\n",
    "\n",
    "    logger = logging.getLogger('base')\n",
    "    logger.info(Logger.dict2str(opt))\n",
    "\n",
    "    tb_logger = SummaryWriter(log_dir=opt['path']['tb_logger'])\n",
    "\n",
    "    logger.info(\"Trial {}: Parameters:\".format(trial.number))\n",
    "\n",
    "    '''\n",
    "    logger.info(\"attn_res: {}\".format(optuna_config[\"model\"][\"unet\"][\"attn_res\"]))\n",
    "    logger.info(\"res_blocks: {}\".format(optuna_config[\"model\"][\"unet\"][\"res_blocks\"]))\n",
    "    logger.info(\"dropout: {}\".format(optuna_config[\"model\"][\"unet\"][\"dropout\"]))\n",
    "    logger.info(\"n_timestep_train: {}\".format(optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"n_timestep\"]))\n",
    "    logger.info(\"n_timestep_val: {}\".format(optuna_config[\"model\"][\"beta_schedule\"][\"val\"][\"n_timestep\"]))\n",
    "    logger.info(\"linear_start_train: {}\".format(optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"linear_start\"]))\n",
    "    logger.info(\"linear_start_val: {}\".format(optuna_config[\"model\"][\"beta_schedule\"][\"val\"][\"linear_start\"]))\n",
    "    logger.info(\"linear_end_train: {}\".format(optuna_config[\"model\"][\"beta_schedule\"][\"train\"][\"linear_end\"]))\n",
    "    logger.info(\"linear_end_val: {}\".format(optuna_config[\"model\"][\"beta_schedule\"][\"val\"][\"linear_end\"]))\n",
    "    logger.info(\"lr: {}\".format(optuna_config[\"train\"][\"optimizer\"][\"lr\"]))\n",
    "    logger.info(\"type_optimizer: {}\".format(optuna_config[\"train\"][\"optimizer\"][\"type\"]))\n",
    "    logger.info(\"finetune_norm: {}\".format(optuna_config[\"train\"][\"optimizer\"][\"finetune_norm\"]))\n",
    "    logger.info(\"which_model_G: {}\".format(optuna_config[\"train\"][\"optimizer\"][\"which_model_G\"]))\n",
    "    '''\n",
    "\n",
    "    wandb_logger = None\n",
    "\n",
    "    # dataset\n",
    "    for phase, dataset_opt in opt['datasets'].items():\n",
    "        if phase == 'train' and opt['phase'] != 'val':\n",
    "            train_set = Data.create_dataset(dataset_opt, phase)\n",
    "            train_loader = Data.create_dataloader(\n",
    "                train_set, dataset_opt, phase)\n",
    "        elif phase == 'val':\n",
    "            val_set = Data.create_dataset(dataset_opt, phase)\n",
    "            val_loader = Data.create_dataloader(\n",
    "                val_set, dataset_opt, phase)\n",
    "    logger.info('Initial Dataset Finished')\n",
    "\n",
    "    # model\n",
    "    diffusion = Model.create_model(opt)\n",
    "    logger.info('Initial Model Finished')\n",
    "\n",
    "    # Train\n",
    "    current_step = diffusion.begin_step\n",
    "    current_epoch = diffusion.begin_epoch\n",
    "    n_iter = opt['train']['n_iter']\n",
    "\n",
    "    if opt['path']['resume_state']:\n",
    "        logger.info('Resuming training from epoch: {}, iter: {}.'.format(\n",
    "            current_epoch, current_step))\n",
    "\n",
    "    diffusion.set_new_noise_schedule(\n",
    "        opt['model']['beta_schedule'][opt['phase']], schedule_phase=opt['phase'])\n",
    "    if opt['phase'] == 'train':\n",
    "        while current_step < n_iter:\n",
    "            current_epoch += 1\n",
    "            for _, train_data in enumerate(train_loader):\n",
    "                current_step += 1\n",
    "                if current_step > n_iter:\n",
    "                    break\n",
    "                diffusion.feed_data(train_data)\n",
    "                diffusion.optimize_parameters()\n",
    "                # log\n",
    "                if current_step >= opt['train']['ema_scheduler']['step_start_ema'] and current_step % opt['train']['ema_scheduler']['update_ema_every'] == 0:\n",
    "                    diffusion.update_lr(opt['train']['ema_scheduler']['ema_decay'])\n",
    "\n",
    "\n",
    "                if current_step % opt['train']['print_freq'] == 0:\n",
    "                    logs = diffusion.get_current_log()\n",
    "                    message = '<epoch:{:3d}, iter:{:8,d}> '.format(\n",
    "                        current_epoch, current_step)\n",
    "                    for k, v in logs.items():\n",
    "                        message += '{:s}: {:.4e} '.format(k, v)\n",
    "                        tb_logger.add_scalar(k, v, current_step)\n",
    "                    logger.info(message)\n",
    "\n",
    "                    if wandb_logger:\n",
    "                        wandb_logger.log_metrics(logs)\n",
    "\n",
    "                # validation\n",
    "                if current_step % opt['train']['val_freq'] == 0:\n",
    "                    avg_psnr = 0.0\n",
    "                    avg_ssim = 0.0\n",
    "                    idx = 0\n",
    "                    result_path = '{}/{}'.format(opt['path']\n",
    "                                                ['results'], current_epoch)\n",
    "                    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "                    diffusion.set_new_noise_schedule(\n",
    "                        opt['model']['beta_schedule']['val'], schedule_phase='val')\n",
    "                    for _,  val_data in enumerate(val_loader):\n",
    "                        idx += 1\n",
    "                        print('feeding data')\n",
    "                        diffusion.feed_data(val_data)\n",
    "                        print('testing diffusion')\n",
    "                        diffusion.test(continous=False)\n",
    "                        visuals = diffusion.get_current_visuals()\n",
    "                        sr_img = Metrics.tensor2img(visuals['SR'])  # uint8\n",
    "                        hr_img = Metrics.tensor2img(visuals['HR'])  # uint8\n",
    "                        lr_img = Metrics.tensor2img(visuals['LR'])  # uint8\n",
    "                        fake_img = Metrics.tensor2img(visuals['INF'])  # uint8\n",
    "\n",
    "                        # generation\n",
    "                        Metrics.save_img(\n",
    "                            hr_img, '{}/{}_{}_hr.png'.format(result_path, current_step, idx))\n",
    "                        Metrics.save_img(\n",
    "                            sr_img, '{}/{}_{}_sr.png'.format(result_path, current_step, idx))\n",
    "                        Metrics.save_img(\n",
    "                            lr_img, '{}/{}_{}_lr.png'.format(result_path, current_step, idx))\n",
    "                        Metrics.save_img(\n",
    "                            fake_img, '{}/{}_{}_inf.png'.format(result_path, current_step, idx))\n",
    "                        \n",
    "                        def ensure_3d(img):\n",
    "                            return np.expand_dims(img, axis=2) if len(img.shape)==2 else img\n",
    "\n",
    "                        fake_img = ensure_3d(fake_img)\n",
    "                        sr_img = ensure_3d(sr_img)\n",
    "                        hr_img = ensure_3d(hr_img)\n",
    "\n",
    "                        tb_logger.add_image(\n",
    "                            'Iter_{}'.format(current_step),\n",
    "                            np.transpose(np.concatenate(\n",
    "                                (fake_img, sr_img, hr_img), axis=1), [2, 0, 1]),\n",
    "                            idx)\n",
    "                        avg_psnr += Metrics.calculate_psnr(\n",
    "                            sr_img, hr_img)\n",
    "                        avg_ssim += Metrics.calculate_ssim(\n",
    "                            sr_img, hr_img)\n",
    "\n",
    "                        if wandb_logger:\n",
    "                            wandb_logger.log_image(\n",
    "                                f'validation_{idx}', \n",
    "                                np.concatenate((fake_img, sr_img, hr_img), axis=1)\n",
    "                            )  \n",
    "\n",
    "                    avg_psnr = avg_psnr / idx\n",
    "                    avg_ssim = avg_ssim / idx\n",
    "                    diffusion.set_new_noise_schedule(\n",
    "                        opt['model']['beta_schedule']['train'], schedule_phase='train')\n",
    "                    # log\n",
    "                    logger.info('# Validation # PSNR: {:.2}'.format(avg_psnr))\n",
    "                    logger.info('# Validation # SSIM: {:.2}'.format(avg_ssim))\n",
    "                    logger_val = logging.getLogger('val')  # validation logger\n",
    "                    logger_val.info('<epoch:{:3d}, iter:{:8,d}> psnr: {:.4e}'.format(\n",
    "                        current_epoch, current_step, avg_psnr))\n",
    "                    # tensorboard logger\n",
    "                    tb_logger.add_scalar('psnr', avg_psnr, current_step)\n",
    "                    tb_logger.add_scalar('ssim', avg_ssim, current_step)\n",
    "\n",
    "                    if wandb_logger:\n",
    "                        wandb_logger.log_metrics({\n",
    "                            'validation/val_psnr': avg_psnr,\n",
    "                            'validation/val_step': val_step\n",
    "                        })\n",
    "                        val_step += 1\n",
    "\n",
    "                if current_step % opt['train']['save_checkpoint_freq'] == 0:\n",
    "                    logger.info('Saving models and training states.')\n",
    "                    diffusion.save_network(current_epoch, current_step)\n",
    "\n",
    "                    if wandb_logger and opt['log_wandb_ckpt']:\n",
    "                        wandb_logger.log_checkpoint(current_epoch, current_step)\n",
    "\n",
    "            if wandb_logger:\n",
    "                wandb_logger.log_metrics({'epoch': current_epoch-1})\n",
    "\n",
    "        # save model\n",
    "        logger.info('End of training.')\n",
    "    else:\n",
    "        logger.info('Begin Model Evaluation.')\n",
    "        avg_psnr = 0.0\n",
    "        avg_ssim = 0.0\n",
    "        idx = 0\n",
    "        result_path = '{}/trial_{}/{}'.format(opt['path']['results'], trial.number, current_epoch)\n",
    "        os.makedirs(result_path, exist_ok=True)\n",
    "        for _,  val_data in enumerate(val_loader):\n",
    "            idx += 1\n",
    "            diffusion.feed_data(val_data)\n",
    "            diffusion.test(continous=True)\n",
    "            visuals = diffusion.get_current_visuals()\n",
    "            #print(type(visuals))\n",
    "\n",
    "            hr_img = Metrics.tensor2img(visuals['HR'])  # uint8\n",
    "            lr_img = Metrics.tensor2img(visuals['LR'])  # uint8\n",
    "            fake_img = Metrics.tensor2img(visuals['INF'])  # uint8\n",
    "\n",
    "            sr_img_mode = 'grid'\n",
    "            if sr_img_mode == 'single':\n",
    "                #print('single')\n",
    "                # single img series\n",
    "                sr_img = visuals['SR']  # uint8\n",
    "                sample_num = sr_img.shape[0]\n",
    "                for iter in range(0, sample_num):\n",
    "                    Metrics.save_img(\n",
    "                        Metrics.tensor2img(sr_img[iter]), '{}/{}_{}_sr_{}.png'.format(result_path, current_step, idx, iter))\n",
    "            else:\n",
    "                # grid img\n",
    "                sr_img = Metrics.tensor2img(visuals['SR'])  # uint8\n",
    "                Metrics.save_img(\n",
    "                    sr_img, '{}/{}_{}_sr_process.png'.format(result_path, current_step, idx))\n",
    "                Metrics.save_img(\n",
    "                    Metrics.tensor2img(visuals['SR'][-1]), '{}/{}_{}_sr.png'.format(result_path, current_step, idx))\n",
    "\n",
    "            Metrics.save_img(\n",
    "                hr_img, '{}/{}_{}_hr.png'.format(result_path, current_step, idx))\n",
    "            Metrics.save_img(\n",
    "                lr_img, '{}/{}_{}_lr.png'.format(result_path, current_step, idx))\n",
    "            Metrics.save_img(\n",
    "                fake_img, '{}/{}_{}_inf.png'.format(result_path, current_step, idx))\n",
    "\n",
    "            # generation\n",
    "            eval_psnr = Metrics.calculate_psnr(Metrics.tensor2img(visuals['SR'][-1]), hr_img)\n",
    "            eval_ssim = calculate_ssim(Metrics.tensor2img(visuals['SR'][-1]), hr_img)\n",
    "            #calculate_ssim(sr_img, hr_img)\n",
    "\n",
    "            avg_psnr += eval_psnr\n",
    "            avg_ssim += eval_ssim\n",
    "            print(eval_ssim)\n",
    "\n",
    "            if wandb_logger and opt['log_eval']:\n",
    "                wandb_logger.log_eval_data(fake_img, Metrics.tensor2img(visuals['SR'][-1]), hr_img, eval_psnr, eval_ssim)\n",
    "\n",
    "        avg_psnr = avg_psnr / idx\n",
    "        avg_ssim = avg_ssim / idx\n",
    "        \n",
    "\n",
    "        logger.info('# Validation # PSNR: {:.4e}'.format(avg_psnr))\n",
    "        logger.info('# Validation # SSIM: {:.4e}'.format(avg_ssim))\n",
    "        logger_val = logging.getLogger('val')  # validation logger\n",
    "        logger_val.info('<epoch:{:3d}, iter:{:8,d}> psnr: {:.4e}, ssim：{:.4e}'.format(\n",
    "            current_epoch, current_step, avg_psnr, avg_ssim))\n",
    "        \n",
    "        tb_logger.add_scalar('psnr', avg_psnr, current_step)\n",
    "        tb_logger.add_scalar('ssim', avg_ssim, current_step)\n",
    "    \n",
    "    return avg_ssim\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")  # Change to 'minimize' if optimizing a loss\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best hyperparameters from the study\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTUNA DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### optuna imports\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "\n",
    "optuna_config = {\n",
    "    \"name\": \"soilCT\",\n",
    "    \"phase\": \"train\",\n",
    "    \"gpu_ids\": [1],\n",
    "    \"distributed\": False,\n",
    "    \"path\": {\n",
    "        \"log\": \"logs\",\n",
    "        \"tb_logger\": \"tb_logs\",\n",
    "        \"results\": \"results\",\n",
    "        \"checkpoint\": \"experiments/checkpoint\",\n",
    "        \"resume_state\": None,\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"train\": {\n",
    "            \"name\": \"soilCT\",\n",
    "            \"mode\": \"LRHR\",\n",
    "            \"dataroot\": \"C:\\\\Users\\\\neuro-ws\\\\data_local\\\\toy_data\\\\train\",\n",
    "            \"datatype\": \"img\",\n",
    "            \"l_resolution\": 32,\n",
    "            \"r_resolution\": 128,\n",
    "            \"batch_size\": 8,\n",
    "            \"num_workers\": 4,\n",
    "            \"use_shuffle\": True,\n",
    "            \"data_len\": 1000,\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"name\": \"soilVal\",\n",
    "            \"mode\": \"LRHR\",\n",
    "            \"dataroot\": \"C:\\\\Users\\\\neuro-ws\\\\data_local\\\\toy_data\\\\val\",\n",
    "            \"datatype\": \"img\",\n",
    "            \"l_resolution\": 32,\n",
    "            \"r_resolution\": 128,\n",
    "            \"data_len\": 2,\n",
    "        },\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"which_model_G\": \"sr3\",\n",
    "        \"finetune_norm\": False,\n",
    "        \"unet\": {\n",
    "            \"in_channel\": 2,\n",
    "            \"out_channel\": 1,\n",
    "            \"inner_channel\": 64,\n",
    "            \"norm_groups\": 32,\n",
    "            \"channel_multiplier\": [1, 2, 4, 8],\n",
    "            \"attn_res\": [32],\n",
    "            \"res_blocks\": 4,\n",
    "            \"dropout\": 0.2,\n",
    "        },\n",
    "        \"beta_schedule\": {\n",
    "            \"train\": {\n",
    "                \"schedule\": \"linear\",\n",
    "                \"n_timestep\": 5000,\n",
    "                \"linear_start\": 1e-8,\n",
    "                \"linear_end\": 1e-2,\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"schedule\": \"linear\",\n",
    "                \"n_timestep\": 5000,\n",
    "                \"linear_start\": 1e-8,\n",
    "                \"linear_end\": 1e-2,\n",
    "            },\n",
    "        },\n",
    "        \"diffusion\": {\"image_size\": 256, \"channels\": 1, \"conditional\": True},\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"n_iter\": 12000,\n",
    "        \"val_freq\": 4000,\n",
    "        \"save_checkpoint_freq\": 12000,\n",
    "        \"print_freq\": 1000,\n",
    "        \"optimizer\": {\"type\": \"adam\", \"lr\": 1e-4},\n",
    "        \"ema_scheduler\": {\"step_start_ema\": 5000, \"update_ema_every\": 1000, \"ema_decay\": 0.9},\n",
    "    },\n",
    "    \"wandb\": {\"project\": \"sr_soilCT\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 12:23:58 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.92                 Driver Version: 545.92       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090      WDDM  | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   43C    P8              19W / 450W |      0MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090      WDDM  | 00000000:03:00.0  On |                  Off |\n",
      "|  0%   41C    P8              30W / 450W |   1152MiB / 24564MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    1   N/A  N/A     12116    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    1   N/A  N/A     12548    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    1   N/A  N/A     12824    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    1   N/A  N/A     12864    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    1   N/A  N/A     13092    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    1   N/A  N/A     15520    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    1   N/A  N/A     16528    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    1   N/A  N/A     17092    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from PIL import Image, ImageFilter\n",
    "import os\n",
    "\n",
    "os.chdir('c:\\\\Users\\\\neuro-ws\\\\data_local\\\\comprs')\n",
    "\n",
    "def calculate_pore_space(image, threshold):\n",
    "    return np.round(np.sum(image < threshold) / image.size * 100, 2)\n",
    "\n",
    "def non_local_mean_filter(image, vals):\n",
    "    j, k, m = vals\n",
    "    # Use OpenCV's fast non-local means denoising function\n",
    "    denoised_image = cv2.fastNlMeansDenoising (image, None, j, k, m)\n",
    "    denoised_image_pil = Image.fromarray(denoised_image)\n",
    "    return denoised_image_pil\n",
    "\n",
    "def downscale_and_filter(image, factor=2, interpolation=cv2.INTER_CUBIC, vals = (20, 7, 21)):\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    upscaled_image = cv2.resize(image_np, dsize = None, fx=1/factor, fy=1/factor, interpolation=interpolation)\n",
    "\n",
    "    filtered_image = non_local_mean_filter(upscaled_image, vals)\n",
    "\n",
    "    return filtered_image\n",
    "\n",
    "# Read the input image\n",
    "input_image_path = \"path/to/your/image.jpg\"\n",
    "input_image = cv2.imread(input_image_path)\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "# Load images in grayscale mode\n",
    "lr = cv2.imread('189_lr.png', cv2.IMREAD_GRAYSCALE) / 255.0\n",
    "hr = Image.open('189_hr.png').convert(\"L\")\n",
    "\n",
    "# Get the size of LR image for resizing\n",
    "lr_size = lr.shape[::-1]\n",
    "\n",
    "# Define resampling methods\n",
    "resample_methods = [Image.BICUBIC, Image.LANCZOS, (1,1,4), (2,1,6), (4,2,8), (8,2,10), (10,5,16)]\n",
    "method_names = ['GAUSSIAN_B','GAUSSIAN_L', 'NON_LOCAL_MEAN', 'NON_LOCAL_MEAN2', 'NON_LOCAL_MEAN3', 'NON_LOCAL_MEAN4', 'NON_LOCAL_MEAN5']\n",
    "\n",
    "# Start plot\n",
    "fig, axs = plt.subplots(1, len(resample_methods)+1, figsize=(27,18))\n",
    "\n",
    "# Show LR image first\n",
    "axs[0].imshow(lr, cmap='gray')\n",
    "axs[0].axis('off')\n",
    "pore_space_orig = calculate_pore_space(np.array(lr), threshold)\n",
    "axs[0].set_title(f'LR \\nPore Space: {pore_space_orig:.2f}%')\n",
    "\n",
    "# For each resampling method, resize HR, calculate metrics and show image\n",
    "\n",
    "for (i), (method, name) in enumerate(zip(resample_methods, method_names), start=1):\n",
    "    # Resize the image\n",
    "    if 'GAUSSIAN' in name:\n",
    "        pyramid = hr.copy()\n",
    "        pyramid_images = [pyramid]\n",
    "        for n in range(2): #(range(3) - x8, range(2) - x4, range(1) - x2) кратность увеличения = степени двойки range\n",
    "            if n == 1 or name == 'GAUSSIAN_L':\n",
    "                pyramid = pyramid.filter(ImageFilter.GaussianBlur(1.9))\n",
    "            pyramid = pyramid.resize((pyramid.width // 2, pyramid.height // 2), method) #Image.LANCZOS #Image.NEAREST\n",
    "            pyramid_images.append(pyramid)\n",
    "        resized_image_np = np.array(pyramid_images[-1]) / 255.0\n",
    "    elif 'NON_LOCAL_MEAN' in name:\n",
    "        pyramid = hr.copy()\n",
    "        pyramid_images = [pyramid]\n",
    "        for n in range(2): #(range(3) - x8, range(2) - x4, range(1) - x2) кратность увеличения = степени двойки range\n",
    "            if n == 1:\n",
    "                pyramid = pyramid.filter(ImageFilter.GaussianBlur(1.8))\n",
    "            pyramid = downscale_and_filter(pyramid, factor=2, vals=method)  \n",
    "            pyramid_images.append(pyramid)\n",
    "        resized_image_np = np.array(pyramid_images[-1]) / 255.0\n",
    "    else:\n",
    "        resized_image = hr.resize(lr_size, method)\n",
    "        resized_image_np = np.array(resized_image) / 255.0\n",
    "\n",
    "    # Calculate PSNR, SSIM, and pore space\n",
    "    psnr_val = psnr(lr, resized_image_np)\n",
    "    ssim_val = ssim(lr, resized_image_np)\n",
    "    pore_space = calculate_pore_space(resized_image_np, threshold)\n",
    "\n",
    "    # Display the image\n",
    "    axs[i].imshow(resized_image_np, cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f'{name}\\nPSNR: {psnr_val:.2f}\\nSSIM: {ssim_val:.2f}\\nPore Space: {pore_space:.2f}%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffbir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
